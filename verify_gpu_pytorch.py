# verify_gpu_pytorch.py
import torch
import sys

print("=" * 50)
print("GPU PyTorch éªŒè¯")
print("=" * 50)

print(f"Pythonç‰ˆæœ¬: {sys.version}")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")

# æ£€æŸ¥CUDA
if hasattr(torch.version, 'cuda') and torch.version.cuda:
    print(f"PyTorch CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"æ˜¯å¦ä¸ºGPUç‰ˆæœ¬: æ˜¯")
else:
    print("PyTorch CUDAç‰ˆæœ¬: æ— ")
    print(f"æ˜¯å¦ä¸ºGPUç‰ˆæœ¬: å¦")

print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
print(f"GPUæ•°é‡: {torch.cuda.device_count()}")

if torch.cuda.is_available():
    print("\nğŸ‰ GPUä¿¡æ¯:")
    for i in range(torch.cuda.device_count()):
        print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
        props = torch.cuda.get_device_properties(i)
        print(f"    æ˜¾å­˜: {props.total_memory / 1024 ** 3:.1f} GB")
        print(f"    è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")

    # æµ‹è¯•GPUè®¡ç®—
    print("\nğŸ§ª GPUæµ‹è¯•:")
    try:
        x = torch.rand(1000, 1000).cuda()
        y = torch.rand(1000, 1000).cuda()
        z = torch.mm(x, y)
        print("  âœ… GPUçŸ©é˜µè¿ç®—æµ‹è¯•é€šè¿‡")
        print(f"  GPUæ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024 ** 2:.1f} MB")
    except Exception as e:
        print(f"  âŒ GPUæµ‹è¯•å¤±è´¥: {e}")
else:
    print("\nâŒ GPUä¸å¯ç”¨")
    print("å¯èƒ½çš„åŸå› :")
    print("1. å®‰è£…çš„ä»æ˜¯CPUç‰ˆæœ¬")
    print("2. CUDAé©±åŠ¨é—®é¢˜")
    print("3. GPUç¡¬ä»¶é—®é¢˜")